# F1 Data Pipeline

This directory contains a standalone data pipeline for scraping and processing Formula 1 documents from the official FIA website.

## Overview

The pipeline is designed to build a comprehensive dataset of [F1 race control documents](https://www.fia.com/documents/championships/fia-formula-one-world-championship-14/season/). It automates the process of downloading official PDF documents and converting them into plain text files, making them suitable for analysis and use in a Retrieval-Augmented Generation (RAG) model.

The pipeline can be run locally to save files to your machine or configured to upload them directly to a Google Cloud Storage (GCS) bucket.

The entire process is orchestrated by `main.py`, which utilizes two core modules: `scraper.py` and `converter.py`.

## Project Structure

- **`main.py`**: The entry point for the data pipeline. It parses command-line arguments to run the scraping and/or conversion steps.
- **`scraper.py`**: Contains the `FIA_Scraper` class, which handles the web scraping process. It navigates the FIA documents website, finds all F1 seasons and events, and downloads the associated PDF documents.
- **`converter.py`**: Contains the `PDF_Converter` class, which is responsible for converting the downloaded PDF files into plain text (`.txt`) files.
- **`data/`**: The default output directory for the pipeline when running locally. It is organized into two subdirectories:
    - `raw_pdfs/`: Stores the original PDF files downloaded by the scraper.
    - `processed_txt/`: Stores the text files generated by the converter.

## Data Directory Structure

Whether running locally or uploading to GCS, the pipeline preserves the `SEASON/EVENT` hierarchy from the FIA website. When using GCS, this structure is replicated within the bucket.

```
data/ or gs://<your-bucket-name>/
├── raw_pdfs/
│   └── SEASON 2024/
│   │   └── Monaco Grand Prix/
│   │       ├── doc_1_-_stewards_decision.pdf
│   │       └── doc_2_-_summons.pdf
|   └── Regulations/
└── processed_txt/
    └── SEASON 2024/
        └── Monaco Grand Prix/
            ├── doc_1_-_stewards_decision.txt
            └── doc_2_-_summons.txt
```

**Note**: pdf files under `Regulations` contains the lastest version of the [FIA Formula One Regulations](https://www.fia.com/regulation/category/110) among all categories. These files are obtained manually directly from the website due to the small amount. Future iterations of implementation plan may automate this process.

## Data Statistics

As of October 2025, the collected dataset contains the following:

-   **Seasons**: 8 (2015, 2019-2025)
-   **Total PDF Documents**: 8708（5 Regulation rule books + 8703 race control documents）

## Setup

This project uses `uv` for Python environment and package management.

1.  **Navigate to the pipeline directory:**
    ```bash
    # From the root of the project
    cd src/datapipeline
    ```

2.  **Create and activate the virtual environment:**
    ```bash
    uv venv datapipeline
    source datapipeline/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    uv pip sync requirements.txt
    ```

## Running the Pipeline

All commands should be run from the `src/datapipeline` directory.

### Local Usage

-   **Scrape Only:**
    To run only the scraper and download all documents locally:
    ```bash
    python main.py --steps scrape
    ```
    To download a limited number of documents for testing:
    ```bash
    python main.py --steps scrape --limit 10
    ```

-   **Convert Only:**
    To run only the converter on already downloaded local PDFs:
    ```bash
    python main.py --steps convert
    ```

-   **Full Run:**
    To run the entire scrape and convert pipeline locally:
    ```bash
    python main.py
    ```

### Google Cloud Storage (GCS) Usage

To upload files to GCS, you first need to set up authentication.

1.  **Authenticate with Google Cloud:**
    Run the following command and follow the instructions to log in:
    ```bash
    gcloud auth application-default login
    ```
    Alternatively, you can use a service account JSON key by setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable.

2.  **Create a GCS Bucket:**
    If you haven't already, create a GCS bucket. You can do this via the Google Cloud Console or with the `gcloud` CLI:
    ```bash
    gcloud storage buckets create gs://<your-bucket-name> --project=<your-project-id>
    ```

3.  **Run the Pipeline with GCS:**
    Use the `--upload_to_gcs` flag to enable GCS mode and specify your bucket with `--bucket`.

    -   **Scrape and Upload to GCS:**
        ```bash
        python main.py --steps scrape --upload_to_gcs --bucket your-bucket-name
        ```

    -   **Convert Files in GCS:**
        ```bash
        python main.py --steps convert --upload_to_gcs --bucket your-bucket-name
        ```

    -   **Full Run with GCS:**
        ```bash
        python main.py --steps all --upload_to_gcs --bucket your-bucket-name
        ```

### GCS Bucket Structure

After running the pipeline with GCS enabled, your bucket will be organized as follows:

![GCS Bucket](assets/GCP.png)

## Containerized Usage with Docker

The data pipeline can be built and run as a containerized application.

1.  **Build the Docker image:**
    From this directory (`src/datapipeline`), run:
    ```bash
    docker build -t f1-data-pipeline -f Dockerfile .
    ```

2.  **Run the container:**
    This command will start the container and give you a shell. The local `data` directory is mounted into the container at `/app/data` to ensure that the downloaded and processed files are saved to your host machine.
    ```bash
    docker run --rm -ti -v $(pwd):/app --name f1-pipeline-container f1-data-pipeline
    ```

    ![Docker Build](assets/docker.png)

3.  **Run a specific step in the container:**
    From the shell inside the container, you can run the pipeline with specific arguments. For example, to run only the scraper with a limit:
    ```bash
    python main.py --steps scrape --limit 20
    ```

    ![Scraper Run](assets/scraper.png)